{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probabilistic Data Structures** represent a relatively new area of algorithms.\n",
    "Notably, the mathematician [Philippe Flajolet](http://algo.inria.fr/flajolet/) is credited with early work, though a number of different people contributed in the examples shown here.\n",
    "\n",
    "You may also hear the terms _approximation algorithms_ or _sketch algorithms_ used to describe the same body of work.\n",
    "These are proving to be especially useful for analytics with large-scale data as well streaming data applications.\n",
    "\n",
    "The main idea here is that these approaches provide *approximations with error bounds*. \n",
    "The amount of acceptable error is generally a trade-off for less system resources, such as memory or compute time.\n",
    "Parameters adjust those error bounds, and also determine the resources required for a given application.\n",
    "\n",
    "This notebook shows Python code for some the more well-known examples of probabilistic data structures, include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|algorithm|usage|\n",
    "|---|---|\n",
    "|HyperLogLog|set cardinality|\n",
    "|BloomFilter|set membership|\n",
    "|MinHash|set similarity|\n",
    "|Count-Min Sketch|frequency summaries|\n",
    "|t-Digest|streaming quantiles|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sets\n",
    "\n",
    "We'll need some interesting data to help illustrate how these kinds of algorithms work. \n",
    "Let's create a data set from the text of [Jabberwocky](http://www.jabberwocky.com/carroll/jabber/jabberwocky.html) by Lewis Carroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jabber_text = \"\"\"\n",
    "`Twas brillig, and the slithy toves\n",
    "  Did gyre and gimble in the wabe:\n",
    "All mimsy were the borogoves,\n",
    "  And the mome raths outgrabe.\n",
    "\n",
    "\"Beware the Jabberwock, my son!\n",
    "  The jaws that bite, the claws that catch!\n",
    "Beware the Jubjub bird, and shun\n",
    "  The frumious Bandersnatch!\"\n",
    "He took his vorpal sword in hand:\n",
    "  Long time the manxome foe he sought --\n",
    "So rested he by the Tumtum tree,\n",
    "  And stood awhile in thought.\n",
    "And, as in uffish thought he stood,\n",
    "  The Jabberwock, with eyes of flame,\n",
    "Came whiffling through the tulgey wood,\n",
    "  And burbled as it came!\n",
    "One, two! One, two! And through and through\n",
    "  The vorpal blade went snicker-snack!\n",
    "He left it dead, and with its head\n",
    "  He went galumphing back.\n",
    "\"And, has thou slain the Jabberwock?\n",
    "  Come to my arms, my beamish boy!\n",
    "O frabjous day! Callooh! Callay!'\n",
    "  He chortled in his joy.\n",
    "\n",
    "`Twas brillig, and the slithy toves\n",
    "  Did gyre and gimble in the wabe;\n",
    "All mimsy were the borogoves,\n",
    "  And the mome raths outgrabe.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we'll use text from another poem, [Daylight Saving](http://www.best-poems.net/dorothy_parker/daylight_saving.html) by Dorothy Parker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parker_text = \"\"\"\n",
    "My answers are inadequate\n",
    "To those demanding day and date\n",
    "And ever set a tiny shock\n",
    "Through strangers asking what's o'clock;\n",
    "Whose days are spent in whittling rhyme-\n",
    "What's time to her, or she to Time?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a simple Python function to construct lists of words from these poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_words (text):\n",
    "  return filter(lambda x: len(x) > 0, re.sub(\"[^A-Za-z]\", \" \", text).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twas', 'brillig', 'and', 'the', 'slithy', 'toves', 'did', 'gyre', 'and', 'gimble', 'in', 'the', 'wabe', 'all', 'mimsy', 'were', 'the', 'borogoves', 'and', 'the', 'mome', 'raths', 'outgrabe', 'beware', 'the', 'jabberwock', 'my', 'son', 'the', 'jaws', 'that', 'bite', 'the', 'claws', 'that', 'catch', 'beware', 'the', 'jubjub', 'bird', 'and', 'shun', 'the', 'frumious', 'bandersnatch', 'he', 'took', 'his', 'vorpal', 'sword', 'in', 'hand', 'long', 'time', 'the', 'manxome', 'foe', 'he', 'sought', 'so', 'rested', 'he', 'by', 'the', 'tumtum', 'tree', 'and', 'stood', 'awhile', 'in', 'thought', 'and', 'as', 'in', 'uffish', 'thought', 'he', 'stood', 'the', 'jabberwock', 'with', 'eyes', 'of', 'flame', 'came', 'whiffling', 'through', 'the', 'tulgey', 'wood', 'and', 'burbled', 'as', 'it', 'came', 'one', 'two', 'one', 'two', 'and', 'through', 'and', 'through', 'the', 'vorpal', 'blade', 'went', 'snicker', 'snack', 'he', 'left', 'it', 'dead', 'and', 'with', 'its', 'head', 'he', 'went', 'galumphing', 'back', 'and', 'has', 'thou', 'slain', 'the', 'jabberwock', 'come', 'to', 'my', 'arms', 'my', 'beamish', 'boy', 'o', 'frabjous', 'day', 'callooh', 'callay', 'he', 'chortled', 'in', 'his', 'joy', 'twas', 'brillig', 'and', 'the', 'slithy', 'toves', 'did', 'gyre', 'and', 'gimble', 'in', 'the', 'wabe', 'all', 'mimsy', 'were', 'the', 'borogoves', 'and', 'the', 'mome', 'raths', 'outgrabe']\n"
     ]
    }
   ],
   "source": [
    "jabber_words = clean_words(jabber_text.lower())\n",
    "print jabber_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'answers', 'are', 'inadequate', 'to', 'those', 'demanding', 'day', 'and', 'date', 'and', 'ever', 'set', 'a', 'tiny', 'shock', 'through', 'strangers', 'asking', 'what', 's', 'o', 'clock', 'whose', 'days', 'are', 'spent', 'in', 'whittling', 'rhyme', 'what', 's', 'time', 'to', 'her', 'or', 'she', 'to', 'time']\n"
     ]
    }
   ],
   "source": [
    "parker_words = clean_words(parker_text.lower())\n",
    "print parker_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some comparisons we'll also need a list of the unique words in one of the poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'and', 'arms', 'as', 'awhile', 'back', 'bandersnatch', 'beamish', 'beware', 'bird', 'bite', 'blade', 'borogoves', 'boy', 'brillig', 'burbled', 'by', 'callay', 'callooh', 'came', 'catch', 'chortled', 'claws', 'come', 'day', 'dead', 'did', 'eyes', 'flame', 'foe', 'frabjous', 'frumious', 'galumphing', 'gimble', 'gyre', 'hand', 'has', 'he', 'head', 'his', 'in', 'it', 'its', 'jabberwock', 'jaws', 'joy', 'jubjub', 'left', 'long', 'manxome', 'mimsy', 'mome', 'my', 'o', 'of', 'one', 'outgrabe', 'raths', 'rested', 'shun', 'slain', 'slithy', 'snack', 'snicker', 'so', 'son', 'sought', 'stood', 'sword', 'that', 'the', 'thou', 'thought', 'through', 'time', 'to', 'took', 'toves', 'tree', 'tulgey', 'tumtum', 'twas', 'two', 'uffish', 'vorpal', 'wabe', 'went', 'were', 'whiffling', 'with', 'wood']\n"
     ]
    }
   ],
   "source": [
    "jabber_uniq = sorted(set(jabber_words))\n",
    "print jabber_uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Cardinality\n",
    "\n",
    "Cardinality is another way of describing how to count the elements in a set.\n",
    "Let's use [HyperLogLog](http://en.wikipedia.org/wiki/HyperLogLog) to implement a probabilistic counter. We'll count the total number of words in one of the poems.\n",
    "\n",
    "The main idea here is that when you have a very large collection of things, counting becomes a problem.\n",
    "In Python, the `long` integers have unlimited precision, so you can count really large sets as long as you have a lot of memory to use.\n",
    "What if you don't want to use up your application's memory?\n",
    "What if you need that memory for other purposes?\n",
    "For example, imagine needing to compare the number of followers on Twitter, when the comparision is between an average user who has about 200 followers and Lady Gaga who has more than 50 million followers.\n",
    "Do you really care whether Lady Gaga has 50,000,000 or 50,000,100 followers?\n",
    "The difference is noise anyway, so why waste lots of application memory?\n",
    "What if there are many celebrities to count and compare?\n",
    "Instead, **approximate**.\n",
    "\n",
    "The *HyperLogLog* algorithm was first described in a [paper by Flajolet](http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf) in 2007.\n",
    "This example uses a [Python implementation](https://github.com/svpcom/hyperloglog) by Vasily Evseenko.\n",
    "\n",
    "Note that the code in this example is configured to accept a 1% counting error. We'll compare the *approximated count* with the *actual count* to measure the observed error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob count 90, true count 91\n",
      "observed error rate 0.01\n"
     ]
    }
   ],
   "source": [
    "import hyperloglog\n",
    "\n",
    "# accept 1% counting error\n",
    "hll = hyperloglog.HyperLogLog(0.01)\n",
    "\n",
    "for word in jabber_words:\n",
    "  hll.add(word)\n",
    "\n",
    "print \"prob count %d, true count %d\" % (len(hll), len(jabber_uniq))\n",
    "print \"observed error rate %0.2f\" % (abs(len(hll) - len(jabber_uniq)) / float(len(jabber_uniq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, those results show how the bounded error rate works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Membership\n",
    "\n",
    "Set Membership determines whether a specified element is known to be within a given set.\n",
    "In other words, once we define the set, we can run *membership queries*.\n",
    "Let's use a [Bloom filter](http://en.wikipedia.org/wiki/Bloom_filter) for approximate membership queries.\n",
    "\n",
    "One interesting property of a *BloomFilter* is that *false negatives* are not possible.\n",
    "So the main idea here is that we can load a set, then test elements to see if they are included in the set.\n",
    "Some small number of elements that aren't in the set will test positive.\n",
    "\n",
    "Suppose you have a database of known customers for your web site, but the queries in the database are expensive.\n",
    "If many people visit your web site, but only a fraction are customers, then you could use a *BloomFilter* of the known customers to test whether you even need to query the database.\n",
    "That could reduce the cost of queries.\n",
    "\n",
    "The *BloomFilter* was first described in [a paper](https://dl.acm.org/citation.cfm?doid=362686.362692) by Burton Bloom in 1970.\n",
    "This example uses a [Python implementation](https://github.com/jaybaird/python-bloomfilter) by Jay Baird.\n",
    "\n",
    "In this case, we'll load the words in *Daylight Saving*, then test the words in *Jabberwocky*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['and', 'in', 'o', 'to', 'through', 'time', 'my', 'day'])\n"
     ]
    }
   ],
   "source": [
    "from pybloom import BloomFilter\n",
    "\n",
    "bf = BloomFilter(capacity=1000, error_rate=0.001)\n",
    "\n",
    "for word in parker_words:\n",
    "  bf.add(word)\n",
    "\n",
    "intersect = set([])\n",
    "\n",
    "for word in jabber_words:\n",
    "  if word in bf:\n",
    "    intersect.add(word)\n",
    "\n",
    "print intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eight words in common.\n",
    "Similarly, this approach can be used in database queries, when one side of `JOIN` is much smaller than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Similarity\n",
    "\n",
    "Thinking about that example above, what if you simply needed an estimate of the number the words in common between the two poems?\n",
    "What if you have many sets and need quick comparisons about how similar they are?\n",
    "\n",
    "For example, suppose you want to test a large number of documents for potential plagiarism?\n",
    "Comparing the similarity of the documents would provide a quick estimate -- enough to weed out the documents that were clearly not similar to each other.\n",
    "\n",
    "The *MinHash* algorithm was first described in [a paper](http://gatekeeper.dec.com/ftp/pub/dec/SRC/publications/broder/positano-final-wpnums.pdf) by Andrei Broder in 1997.\n",
    "This example uses a [Python implementation](https://github.com/ekzhu/datasketch) by Eric Zhu.\n",
    "\n",
    "Here we'll estimate the similarity between the words in the two poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard simularity 0.074219 estimated\n",
      "Jaccard simularity 0.069565 actual\n"
     ]
    }
   ],
   "source": [
    "from hashlib import sha1\n",
    "from datasketch import MinHash\n",
    "\n",
    "def mh_digest (data):\n",
    "  m = MinHash(num_perm=512)\n",
    "\n",
    "  for d in data:\n",
    "    m.digest(sha1(d.encode('utf8')))\n",
    "\n",
    "  return m\n",
    "\n",
    "m1 = mh_digest(set(jabber_words))\n",
    "m2 = mh_digest(set(parker_words))\n",
    "\n",
    "print \"Jaccard simularity %f\" % m1.jaccard(m2), \"estimated\"\n",
    "\n",
    "s1 = set(jabber_words)\n",
    "s2 = set(parker_words)\n",
    "actual_jaccard = float(len(s1.intersection(s2)))/float(len(s1.union(s2)))\n",
    "\n",
    "print \"Jaccard simularity %f\" % actual_jaccard, \"actual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An estimate of 7.4% similarity versus a measure of 7.0% actual similarity, based on the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index).\n",
    "In any case, we're fairly certain that Dorothy Parker didn't plagiarize Lewis Carroll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Summaries\n",
    "\n",
    "Frequency Summaries are another way of describing the frequency of events in a data stream.\n",
    "Think: leaderboards used for online competitions, etc.\n",
    "\n",
    "The main idea is that we want to measure and compare which events are occurring most frequently, and we probably don't care about the events that occur less often.\n",
    "The precision of the frequencies isn't particularly imporant.\n",
    "\n",
    "We'll use [Count-min sketch](http://en.wikipedia.org/wiki/Count%E2%80%93min_sketch) for frequency summaries, to implement a probabilistic [word count](http://en.wikipedia.org/wiki/Word_count) on one of the poems.\n",
    "\n",
    "The *Count-Min Sketch* algorithm was first described in [a paper](http://dl.acm.org/citation.cfm?id=1073718) by Graham Cormode and S. Muthukrishnan in 2005.\n",
    "This example uses a [Python implementation](https://github.com/IsaacHaze/countminsketch) by Isaac Sijaranamual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from yacms import CountMinSketch\n",
    "\n",
    "counts = Counter()\n",
    "\n",
    "# table size=1000, hash functions=10\n",
    "cms = CountMinSketch(200, 3)\n",
    "\n",
    "for word in jabber_words:\n",
    "  counts[word] += 1\n",
    "  cms.update(word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the counts for common words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances of the word `the` 19\n",
      "instances of the word `and` 14\n",
      "instances of the word `he` 7\n",
      "instances of the word `that` 2\n"
     ]
    }
   ],
   "source": [
    "for word in [\"the\", \"and\", \"he\", \"that\"]:\n",
    "  print \"instances of the word `%s` %d\" % (word, cms.estimate(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One practical example would be in language detection: comparing frequencies for the most commonly occuring words is a simply way to predict the language in which a document was written.\n",
    "\n",
    "Next, let's look at where the estimates in the *sketch* differed from actual counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed 'galumphing' counter: 1, sketch: 2\n",
      "missed 'head' counter: 1, sketch: 2\n",
      "missed 'left' counter: 1, sketch: 2\n"
     ]
    }
   ],
   "source": [
    "for e in counts:\n",
    "  if counts[e] != cms.estimate(e):\n",
    "    print \"missed '%s' counter: %d, sketch: %d\" % (e, counts[e], cms.estimate(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bokay, that was way better than writting 50 lines of Java code for a Hadoop application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Quantiles\n",
    "\n",
    "Imagine that you have a large stream of data.\n",
    "Perhaps you have an application that must measure that stream continually, where stopping to count simply isn't an option.\n",
    "Suppose you're monitoring ecommerce transations, trying to detect credit card fraud?\n",
    "Some measure become important (e.g., average order price).\n",
    "Approximating metrics on a stream is a great way to build applications that are more robust and operate in real-time.\n",
    "\n",
    "The *t-Digest* algorithm was first described in [a paper](https://github.com/tdunning/t-digest/blob/master/docs/theory/t-digest-paper/histo.pdf) by Ted Dunning and Otmar Ertl in 2013. \n",
    "This example uses a [Python implementation](https://github.com/trademob/t-digest) by Trademob GmbH.\n",
    "\n",
    "In this case, we'll skip the poems.\n",
    "Instead let's generate a stream of random numbers and look at its *head*, *median*, and *tail*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050000 @ 0.046682\n",
      "0.500000 @ 0.501629\n",
      "0.950000 @ 0.947825\n"
     ]
    }
   ],
   "source": [
    "from tdigest import TDigest\n",
    "import random\n",
    "\n",
    "td = TDigest()\n",
    "\n",
    "for x in xrange(0, 1000):\n",
    "    td.add(random.random(), 1)\n",
    "\n",
    "for q in [0.05, 0.5, 0.95]:\n",
    "    print \"%f @ %f\" % (q, td.quantile(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo-random number generator in Python provides something close to a *uniform distibution*.\n",
    "In other words, it's flat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "This is just a start. Even so, not so many programmers can claim that they have written and run code that takes advantage of probabilistic data structures. You have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider how this work contrasts with statistics.\n",
    "In statistical modeling, one fits a data set to a known *probability distribution*, then makes inferences from the model. For example, \n",
    "Generally that co\n",
    "\n",
    "\n",
    "\n",
    "Twitter catch-phrase is: “Hash, don’t sample”\n",
    "\n",
    "aggregate different ranges by composing the hashes, instead of repeating full-queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely check out this O'Reilly webcast, [Probabilistic Data Structures and Breaking Down Big Sequence Data](http://www.oreilly.com/pub/e/1784) by C. Titus Brown, about using approximation algorithms in genomics.\n",
    "\n",
    "[Add ALL The Things](http://www.infoq.com/presentations/abstract-algebra-analytics) by Avi Bryant @ Stripe.\n",
    "\n",
    "Algebird\n",
    "\n",
    "MMDS\n",
    "\n",
    "BlinkDB\n",
    "\n",
    "[Probabilistic Data Structures for Web Analytics and Data Mining](http://highlyscalable.wordpress.com/2012/05/01/probabilistic-structures-web-analytics-data-mining/) by Ilya Katsov."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
